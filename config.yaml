# Pipeline Configuration

# Data acquisition
data_dir: data/raw
data_urls:
  - https://s3.us-east-1.amazonaws.com/mainpipe.maincode.com/mainpipe_data_v1.jsonl
local_files: []
# - data/raw/sample.jsonl

# Cleaning and normalization
cleaning:
  min_length: 50
  max_length: 100000
  remove_urls: false
  remove_emails: false

# Language filtering
language: en
allowed_languages:
  - en
  - es
  - fr
lang_confidence: 0.7

# Duplicate detection
remove_duplicates: true
dup_method: exact  # exact, fuzzy, hash

# PII detection
remove_pii: false
pii_threshold: 0.5

# Tokenization
tokenizer_type: bpe  # bpe, wordpiece, pretrained
vocab_size: 30000
train_tokenizer: true
# pretrained_tokenizer: bert-base-uncased
# tokenizer_path: data/tokenizer.json

# Export settings
output_dir: data/processed
reports_dir: data/reports
shard_size: 10000

# Logging
log_level: INFO

# Development/test helpers
# Limit the number of texts processed for quick local tests. Set to null to process all.
sample_size: 200
# Number of workers to use for processing. Set to 1 to run single-threaded.
workers: 4
# parallel_mode: 'process' will use multiprocessing (may duplicate models per process).
# parallel_mode: 'thread' will use threads (shares model instances, safer when spaCy/presidio are used).
parallel_mode: thread
