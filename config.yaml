# Pipeline Configuration

# Data acquisition
data_dir: data/raw
data_urls:
  - https://s3.us-east-1.amazonaws.com/mainpipe.maincode.com/mainpipe_data_v1.jsonl
local_files: []
# - data/raw/sample.jsonl

# Cleaning and normalization
cleaning:
  min_length: 50
  max_length: 100000
  remove_urls: false
  remove_emails: false

# Language filtering
language: en
allowed_languages:
  - en
lang_confidence: 0.7

# Duplicate detection
remove_duplicates: true
dup_method: minhash  # exact (SHA-256), minhash (Lee et al. 2022), hash
# MinHash parameters (only used if dup_method=minhash)
minhash_num_perm: 128        # Number of hash permutations (higher = more accurate, slower)
minhash_threshold: 0.8       # Jaccard similarity threshold (0.8 = 80% similar)
minhash_ngram_size: 5        # Character n-gram size (5 recommended for English)

# PII detection
redact_pii: false  # Redact PII by replacing entities with <TYPE> placeholders (e.g. <PERSON>, <EMAIL>)
pii_threshold: 0.5
# inspect_pii controls whether to run PII detection for reporting/inspection
# Set to false to skip PII detection entirely (much faster!)
inspect_pii: false
# Use smaller/faster spaCy model for PII detection (en_core_web_sm is 10x faster than en_core_web_lg)
spacy_model: en_core_web_sm
# pii_batch_size: Number of texts to process together in PII detection (higher = faster but more memory)
pii_batch_size: 200

# Tokenization
tokenizer_type: bpe  # bpe, wordpiece, pretrained
vocab_size: 30000
train_tokenizer: true
# pretrained_tokenizer: bert-base-uncased
# tokenizer_path: data/tokenizer.json

# Export settings
output_dir: data/processed
reports_dir: data/reports
shard_size: 10000

# Logging
log_level: INFO

# Development/test helpers
# Limit the number of texts processed for quick local tests. Set to null to process all.
sample_size: 10000
# Process data in chunks to save memory and enable checkpointing
# 10K chunks = ~27 chunks for 269K texts, checkpoint saved after each
chunk_size: 2000
# Number of workers to use for processing. Set to 1 to run single-threaded.
# For thread mode, can use more workers (e.g., 8-16) since threads are lightweight
workers: 4
# parallel_mode: 'process' will use multiprocessing (may duplicate models per process).
# parallel_mode: 'thread' will use threads (shares model instances, safer when spaCy/presidio are used).
# For large datasets without PII removal, 'process' is faster
# IMPORTANT: Use 'thread' for duplicate detection to work across chunks
parallel_mode: process
